# Query理解架构

## **1.搜索引擎**

一个基本的搜索系统大体可以分为离线挖掘和在线检索两部分，其中包含的重要模块主要有：Item内容理解、Query理解、检索召回、排序模块等。整个检索系统的目标可以抽象为给定query，检索出最能满足用户需求的item

## 1.1 **离线挖掘**

在离线侧，需要做一些基础的离线挖掘工作，包括item内容的获取、清洗解析、item内容理解（语义tag、权威度计算、时间因子、质量度等）、用户画像构建、query离线策略挖掘、以及从搜索推荐日志中挖掘item之间的语义关联关系、构建排序模型样本及特征工程等。进行item内容理解之后，对相应的结构化内容执行建库操作，分别构建正排和倒排索引库。其中，正排索引简单理解起来就是根据itemid能找到item的各个基本属性及term相关（term及其在item中出现的频次、位置等信息）的详细结构化数据。

相反地，倒排索引就是能根据分词term来找到包含该term的item列表及term在对应item中词频、位置等信息。通常会对某个item的title、keyword、anchor、content等不同属性域分别构建倒排索引，同时一般会根据item资源的权威度、质量度等纵向构建分级索引库，首先从高质量库中进行检索优先保证优质资源能被检索出来，如果高质量检索结果不够再从低质量库中进行检索。为了兼顾索引更新时效性和检索效率，一般会对索引库进行横向分布式部署，且索引库的构建一般分为全量构建和增量更新。常见的能用于快速构建索引的工具框架有Lucene全文检索引擎以及基于Lucene的Solr、ES（Elastic Search）等。

除了基本的文本匹配召回，还需要通过构建query意图tag召回或进行语义匹配召回等多路召回来提升搜索语义相关性以及保证召回的多样性。

## 1.2**在线检索**

线上执行检索时大体可以分为基础检索（BS）和高级检索（AS）两个过程，其中BS更注重term级别的文本相关性匹配及粗排，AS则更注重从整体上把控语义相关性及进行精排等处理。首先会去请求SearchQU服务（负责搜索query理解）获取对query理解后的结构化数据，然后将这些结构化数据传给基础召回模块BS，BS根据切词粒度由粗到细对底层索引库进行一次或多次检索，执行多个索引队列的求交求并拉链等操作返回结果。

同时BS还需要对文本、意图tag、语义召回等不同路召回队列根据各路召回特点采用多个相关性度量（如：BM25文本相似度、tag相似度、语义相关度、pagerank权威度、点击调权等）进行L0粗排截断以保证相关性，然后再将截断后的多路召回进行更精细的L1相关性融合排序，更复杂一些的搜索可能会有L0到LN多层排序，每层排序的侧重点有所不同，越高层次item数变得越少，相应的排序方法也会更复杂。



## 2.Query理解

![img](https://netease-we.feishu.cn/space/api/box/stream/download/asynccode/?code=79e840243342dec67c39a77b879675b9_8f118824ce50c961_boxcnq0IQe7SUedqShAkwbnrVlg_GG0u4KfPhC2UkncJD7vfA2yxizmJZYZP)

1）用户输入的Query，先去查询下cache，看是否命中，命中就返回，否则继续（cache看实际情况，一般加cache是为了缓解线上服务的压力，但也有它的问题，比如：更新不及时，cache过期问题等）。



2）Query预处理，全半角转换，繁简体，无用字符去掉，过长query截断等简单处理。



3）中文分词，是query理解的基础模块，分词按照粒度可以分为基本粒度切分，词语切分和短语切分，基本粒度是最小的粒度切分，是为了保证数据召回的，一般索引采用基本粒度切分，检索一般按照分级检索召回策略，从最粗粒度：短语切分开始，如果召回不理想，再进行词语切分召回，最后才是基本粒度召回。



4）Query改写，包括query纠错，归一化和扩展功能，query纠错会对用户的输入错误，词语错误，同音词错误等进行纠错。query归一化会query进行统一处理，比如字符的大小写统一，同义词处理等等。



5）Query分析，基于中文分词，会对query切分成term的词性，词法结构，term重要性和紧密度进行分析，并可以把不重要的term忽略或者做query语法“或”处理。



Term紧密度

主要用于衡量query中任意两个term之间的紧密程度，如果两个term间紧密度比较高，则这两个term在召回item中出现的距离越近相对来说越相关。以相邻的两个term为例，由于分词工具本身存在准确率问题，某两个term可能不会被分词工具切分到一起，但它们之间的关系比较紧密，也即紧密度比较高，如果在召回时能将在item中同时出现有这两个term且出现位置挨在一起或比较靠近的item进行召回，相关性会更好。

在进行召回时，传统的相关性打分公式如OkaTP、BM25TP、newTP、BM25TOP等在BM25基础上进一步考虑了proximity计算，但主要采用两个term在item中的距离度量，如：

![img](https://netease-we.feishu.cn/space/api/box/stream/download/asynccode/?code=eab3ff70c6ed32a8daeea9446f4e9444_8f118824ce50c961_boxcncMhdPkyXMEN04wHujQ6PLh_NdZcvoqtQEXGxMs9y6n8YWxHJycvt8pK)

 有了query中的term紧密度，在召回构造查询索引的逻辑表达式中可以要求紧密度高的两个term需共同出现以及在proximity计算公式中融合考虑进去，从而保证query中紧密度高的两个term在item中出现距离更近更相关。

**Term重要性**

特征方面，则可以从词法、句法、语义、统计信息等多个方向进行构造，比如：term词性、长度信息、term数目、位置信息、句法依存tag、是否数字、是否英文、是否停用词、是否专名实体、是否重要行业词、embedding模长、删词差异度、前后词互信息、左右邻熵、独立检索占比（term单独作为query的qv/所有包含term的query的qv和）、iqf、文档idf、统计概率 以及短语生成树得到term权重等。

其中删词差异度的做法是先训练得到query的embedding表示，然后计算移除各个term之后的query与原query的embedding相似度差值用于衡量term的重要性，如果移除某个term之后相似度差异很大，代表这个term比较重要。

对于重要级别最低的term可以考虑直接丢词，或者在索引库进行召回构造查询逻辑表达式时将对应的term用“or”逻辑放宽出现限制

6）意图识别，分为精确意图和模糊意图，精确意图的query比较好处理，模糊意图需要的策略比较多，比如意图分类，聚类分析，位置分析等等。

https://zhuanlan.zhihu.com/p/112719984





## 知乎搜索框背后的Query理解和语义召回技术

![image-20201219095507437](C:\Users\yuqin03\AppData\Roaming\Typora\typora-user-images\image-20201219095507437.png)

知乎的召回系统主要分为两部分：

❶ 基于词的传统的倒排索引召回；

❷ 基于向量的向量索引的召回机制。

如图所示，实际召回中有三个队列，基于倒排索引的队列占两个，分别是原始 Query 队列和改写 Query 队列。<font color='red'>改写队列是用翻译模型去生成一个表达更适合检索的 query，然后再去做 query 理解和索引召回。</font>而第三个队列是基于 embedding 的索引。首先把所有 doc、原始 query 都转为向量，再通过空间最近邻的 KNN-Search，找到与 query 最相似的文档。

最终参与排序的就是这三个队列，并进行合并和精排，在精排之后会进行上下文排序。下

基于词的传统的倒排索引召回:

term weight，最简单的实现方式是用 IDF 逆文档频率算出,局限是无法根据 query 上下文语境动态适应，。但是term的重要程度并不是和term的出现次数呈严格单调关系，并且idf缺乏上下文语境的考虑（比如“windows”在“windows应用软件”中比较重要，而在“windows xp系统iphone xs导照片”的重要性就比较低因为其在不同的上下文中 weight 是一致的.所以我们再通过统计点击数据中进行调整。如图，如果用户的 query 包含 a，b 两个 term，并且点击了 Doc1 和 Doc4，其中 Doc1 包含 a，b， Doc4 只包含a，即 a 出现2次，b 出现1次。一个朴素的想法就是 a 的权重就是1，b 是0.5，这样就可以从历史日志里把每个 query 里的 term 权重统计出来。

这个方法对于从未出现过的 query 就无法从历史数据中统计得到。所以我们从 query 粒度泛化到 gram 粒度，按 ngram 来聚合。比如 a，b 就是一个 bigram，类似于 mapreduce 的 map 过程，首先会把 query 中的 ngram 先 map 出来，再发送到 reduce 里面统计。这样就能统计出每个 ngram 下较置信的权重。

对于 ngram，在实际操作中需要注意以下几点：

❶ 词表较大，往往在十亿量级以上；

❷ 字典更新较慢；

❸ 无法处理长距离依赖。基本最多到 3gram，多 gram 词不仅会使词表更加膨胀，词与词的依赖关系也将更难捕捉到；

❹ 无法解决过于稀疏的词，过于长尾的词会退化成 idf。

我们的解法是把 **ngram 基于词典的泛化方法变为基于 embedding 的模型泛化方法。**目标是想得到根据 **query 语境动态自适应的 term weight**。如果可以获取跟 query 上下文相关的动态词向量，那么在词向量的基础上再接 MLP 就可以预测出词权重了。**目前的方法是通过 term 词本身的词向量减去 query 所有词 pooling 之后的词向量获得，今后也会尝试替换成 ELMo 或者 Transformer 的结构以获得更好的效果。**

https://zhuanlan.zhihu.com/p/158061596  医疗搜索中的Query词权重算法探索



## **语义召回**

按照经验来说，interaction based model 效果会好于 representation based model。但是在向量召回这个任务当中，我们需要离线计算好所有文档的 embedding 并建成向量索引，所以语义召回采用的的是 representation based 模型

借鉴经典的孪生网络结构，我们用 BERT 模型分别做 query 和 doc 的 encoder，接着用 pooling 之后的 query doc cosine 距离当做输出，最后通过 pairwise 损失函数训练模型。在 pooling 方法上，我们也尝试过不同层 pooling，或者在多层上增加 attention 聚合 BERT 多层结果，效果和最后一层所有 token 做 average pooling 相当。损失函数方面，因为我们是做召回阶段的模型，所以参考过雅虎的方法，他们认为在一轮排序阶段使用 pointwise 损失函数的效果会更好，但是在我们的数据上 pairwise 还是会稍好一

 数据增强，增加一些对于模型来说更难区分的负样本

![image-20201219100127658](C:\Users\yuqin03\AppData\Roaming\Typora\typora-user-images\image-20201219100127658.png)

用原始的脚本数据去微调一个模型，计算 doc 的 embedding 和 query 的 embedding。对 doc embedding 进行聚类，此时 doc 和 query 的 embedding 就在同一空间里，所以 query embedding 就可以对应到某一个类别。这时候即可从 doc 里直接采样一些负样本了。

补充

![image-20201219100417352](C:\Users\yuqin03\AppData\Roaming\Typora\typora-user-images\image-20201219100417352.png)

Word2vec 这一类的 embedding 计算方法，核心假设是拥有相似上下文的词相似。需要注意该方法无法区分同义词和反义词/同位词，例如最大/最小，苹果/香蕉。我们这里利用监督学习的信号，对训练完成的词向量空间进行微调。

❹ 外部数据

用一些规则如百科里 "XXX 又名XXX" 句式进行挖掘。

###  query 改写部分。为什么要做 query 改写？

在传统搜索里，一个 query 进来需要做多个子任务的处理。假如每个任务只能做到90%，那么积累起来的损失会非常大。我们需要一个方法一次性完成多个任务去减小损失，比如把 "苹果手机价格多少" 直接改写为 "苹果手机售价"。

用翻译方法可以实现这个需求，即把改写问题看成 src 和 target 语言为同一种语言的翻译问题，翻译模型用的是谷歌的 NMT 结构。

对于这个任务来说，训练语料的挖掘比修改模型结构更为重要，我们通过用户行为日志来挖掘训练语料。比如从 query 到 title，点击同一 doc 的不同 query，都可形成平行语料。

最后我们用来训练模型的语料用的是 Query -> Query 的平行语料，因为两者在同一个语义空间，长度也基本接近。如果 query 翻译成 title，则会因为 title 长度通常过长而加大训练复杂度。具体方法是：

<font color='red'>**❶ 语言模型的过滤**

**目的是把罕见的表达换为常见的表达。用 ngram 模型算一个得分，把较罕见的放在前面；**

**❷ 进行相关性过滤**

**考虑到点击日志噪声比较多，通过相关性过滤掉噪声；**

**❸ 确定切词粒度**

**用的是 BPE 的 Subword 方法。未用知乎内部的切词是因为未登录词无法预测，同时词表较大，训练速度较慢。</font>**

