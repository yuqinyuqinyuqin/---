# Query词权重算法

当Query存在多个词时，每个词的重要程度是不同的，在基于词召回的基础上，对词设立 不同重要程度，以此达到近似Query原始的语义

考虑Query为“Paris news”，索引的构建通常是以词为颗粒度Query中的“Paris”和“news”在一般情况下是等价的，系统并没有倾向性。当然，主流的搜索引擎（比如ES）会利用一些统计指标做些优化（比如常见的BM25算法），但是这需要文档集中各个词分布符合理想预期。比如说，理想情况下“Paris”的逆文档频应该比“news”更高，但是在很多case上是无法满足，Query词权重计算想要解决的就是弥补全局统计量精度缺失的问题.

### **一. 基于统计的词权重模型**

基于query和document之间的统计特征（词频， 共现，点击等），得到query的词权重分布，是直接且效果最显著的方法。

###  二、**基于关键词提取的词权重算法**

### 三、神经网络训练

[Context-Aware Sentence/Passage Term Importance Estimation For First Stage Retrieval]

​	在query-doc任务中，一般在召回层返回给我们一些文档(数量级在百），我们需要再对其进行排序返回跟query最相关的doc（数量级在十），第一阶段的召回传统做法是基于布尔查询的，当一个query中有多个term时，每个词对于文档的召回的贡献度是不一样的，我们不能一视同仁地看待这些term，常用做法是基于统计得到的词频，逆文档频率。但基于频率得到的term 权重并不一定对应term在这段话中的重要性。

#### 1. Contextualized word embedding generation:生成上下文敏感的向量，这部分没啥好说的~如果不太理解，回去看下Bert模型哦

#### 2.用一个线性回归函数来将基于上下文的向量转化为term weight，损失函数为MSE,

![img](https://img2020.cnblogs.com/blog/2219299/202012/2219299-20201219204358870-1525591868.png)

 

真实label下面介绍哈

Bert模块用训练好的Bert模型初始化，最后一层的回归层的参数从头学起。

我们可以用这篇文章提出的DeepCT 做以下两件事：

1、对片段或者片段长短的文档进行词权重计算、通过倒排进行存储每个term对应的权重

在这个任务中，要对片段中的term 进行权重学习，回归层的真实label采用的是query term recall，

![img](https://img2020.cnblogs.com/blog/2219299/202012/2219299-20201219210828038-529280693.png)

 

 怎么理解这个指标呢？假设我们面前摆了一段话，你对这段话进行提问，在提问中出现的越多，也就证明这个词在这段话的重要性越高。

前面线性转换层输出是每个term对应的weight，数值在【0，1】，乘上一个系数进行放大，

![img](https://img2020.cnblogs.com/blog/2219299/202012/2219299-20201219205614942-1088346857.png)

 

 2、对query中的每个term 赋予权重

对于含有很多term，很长的query，如果我们用传统的TF,每个词的tf会很接近拉不开差距，

当线上接收到一个query时，比如“apple pie",我们会用已经估计好的term weight来生成bag -of -word query , 会得到

 weight(0.8 apple 0.7 pie)，bow未考虑到单词之间的次序，引入次序信息的话和term之间的共线性，就是Sequential dependency *model。*

###### 

丁香园 ：模型采用基于知识表示的textRank模型，利用实体词的知识表示（实体和关系）加重实体词在query中的重要性，同时也采用了一些统计特征，如tfidf，属性词表，停用词表等，降低其他词的权重，从而拉大词之间的权重差距。为辅助排序效果，对原始的权重进行TFDeepCT策略，进一步拉开差距，但是对原本分词效果不好（“青岛滨海学院附属医院”应该是一个词，但是分词结果为“青岛 滨海 学院 附属医院”，这导致最终结果会对这几个词中的某一个词着重搜索）和词差距不大的query（2019考研试题，这实际上算是一个名词短语，其实并不存在谁最关键）不进行TFDeepCT。同时为使用更多的统计特征，上述模型基础上复现MIKE，加入了点击，主题分布，query-搜索结果共现等特征。